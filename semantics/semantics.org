#+TITLE: A Brief Summary Of Semantics
#+AUTHOR: KW

# Note: this is more like a mind-map than a set of notes, really. 

Computational semantics. I've put my notes on this here: [computational semantics notes???]. Here are a few computational semantic models in current usage, almost all of them are terrible.

Or so I think, anyway.

# oh thank fuck I didn't lose my notes

* Frame Semantics
* Segueing From That: Compositional Typed Semantics
* Event Semantics

** Types of events

*** Telic

Set terminal point of event.

**** Activity

/for/ - adverbial

**** Stative

/for/ - adverbial

*** Atelic

No set terminal point of event.

**** Achievement

/in/ - adverbial

**** Accomplishment

/in/ - adverbial
** Subevents

Events can be decomposed into subevents. A very nice example of this can be seen with the Kaaraka-based parsing of the sentence:

"Aaurat ne bacche ko doodh pilaya."

Subevents: "pilaya - Aaurat --> bacche"
           "piya - baccha --> doodh"

An event can also be broken down into subevents in the sense of having an internal process, a terminal state, a consequent state, etc. For example:

    ACTIVITY denotes a process
    ACCOMPLISHMENT denotes a process leading up to a result state

** Arguments of events

*** Arguments vs. adjuncts

We went over this with reference to X-bar structures already.

* Distributional Semantics

Defining the semantics of a word (or any linguistic element, really), with respect to its relationships with other elements /in a text/. (Well, obviously, where else do we mine the information from?)

# N: A knowledgebase. 

** Vector Semantics

    - Words are represented as vectors in a high-dimension vector space
    - *Semantic similarity* is measured using geometrical space between the word-vectors
    - The values within a vector matrix of a word are the occurrences of that word within given context (i.e: each 'dimension' of the vector space is a particular context)

*** Skip-Gram
*** CBOW
** Graph Semantics

The basic idea is: stop using vectors, start using graphs with weighted edges.
Why would we do this?

*** Well, there are a few advantages

    - Reduces time complexity of algorithms because you have to iterate over a graph instead of a matrix
    - A vector space, theoretically, can have an infinite number of dimensions. A graph just needs to store edges and edge weights.
    - Dense vector spaces, in particular, suffer from information loss as the amount of information about a particular word is equal to the number of dimensions. Some information will be lost. Say, a minor distinction between two words based on some obscure bit of context might be lost if we prune the number of dimensions to a decent size.
    - Multiple types of edges, multiple types of pair-relations. This is demonstrated by the /@/ relation: instead of simple contexts, we can define the relation of words to each other based on, say, dependencies.
    - I really don't know what MapReduce has to do with anything, but it 'somehow' scales without using a lot of RAM. Somehow. Take it on trust.
    - Can easily make a *Distributional Thesaurus* based on commonality of (ranked) neighbours
    - Similarity: can retrieve most similar words (based on a common *semantic neighbourhood*) much easier than a vector-representation-system can do it. Also, the triangle inequality does not hold with graph-based similarity calculations, while it holds for vector-based similarity calculations. Just think about it:

D(a,b) + D(a,c) >= D(b,c)

So, suppose we wanted to calculate similarity between three words, a will always be more similar to both b and c than b and c will be to each other.

I think. I'm just bullshitting here.

    - With a graph-based representation, you really cannot compare words that don't have a lot in common. With a vector representation, you can compare anything, including ridiculous comparisions.
    - Looks pretty: you can represent it like a nice, zoomable graph, where the most common words are displayed at lower resolutions, and split into a component 'subgraph' so to speak when zoomed in. 

*** Graph-Based Sense-Clustering Using Chinese Whisper Algorithm

The paper on this is somewhere in this directory. It's a pretty nice paper, do give it a read.

**** Nice things about it

    - Time-linear in the number of edges. Nice.

**** Chinese Whisper Algorithm for general G(V, E)

First, we describe how the algorithm works on a general graph, then show how it also works in sense-clustering.

#+BEGIN_EXAMPLE

  initialisation step :
      for all v belonging to V :
          class(v) = <random>

  clustering step :
      while there are changes in node classes:
          for all v in V, selected in random order :
              class(v) = highest ranked class in the neighbourhood of v
              where
                  rank(class_i) <-- forAll nodes in neighbourhood of v belonging to class_i:
                                      return sum(weights of edges from nodes to v)
     
#+END_EXAMPLE
    
Note: the number of iterations in the clustering step needed before classes stop changing, depends on the diameter of the graph (i.e: the maximum number of edges on a shortest path between two nodes).

**** Chinese Whisper Algorithm For WSI

We perform CW on a co-occurrence graph. 

I don't know if it's the same co-occurrence graph as described in [[./words_co_occur_in_list.pdf][this paper]] (just look at the "Building a graph on similar words" section- it describes how they add edges between words that specifically co-occur in a list, like say "bread, milk and eggs"- that sort of list), or just a normal co-occurrence graph. Here's what the paper says:

#+BEGIN_QUOTE

Similar to the approach as presented in (Dorow
and Widdows, 2003) we construct a word graph.
While there, edges between words are drawn iff
words co-occur in enumerations, we use the co-
occurrence graph.

#+END_QUOTE

God knows what the fuck that means.

Anyway, to automatically derive the multiple senses of a single word, they:

(1) Take the sub-graph formed by the neighbourhood (all nodes connected to) that word.
(2) Run CW on this sub-graph.
(3) Assume that each resultant cluster represents a single sense.
