#+TITLE: Deep Contextualised Word Representations- And Other Things
#+AUTHOR: CL

* Deep Contextualised Representations

pretrained word vectors, but instead of using just the top hidden layer they're using deeper layers of the LSTM that trains on unlabeled data.
(so basically word2vec with more layers?)

** Why Deeper Layers

   - It's been shown (by application in specific tasks) that different layers encode different information 
   - Lower levels of layers - syntactic information
   - Upper levels - semantic and contextual information
* The Bidirectional LSTM

  - Forward: given a sequence of =n= tokens, model the probability of token =t_k= given the history of tokens =t_1= to =t_k=
  - Backward: given a reverse sequence of tokens =t_k= to =t_2=, predict token =t_1=
  - biLM: [(forward repr.), (backward repr.)]
