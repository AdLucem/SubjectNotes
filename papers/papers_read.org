#+TITLE: Papers Read

An ordered repository of papers I've read/am intending to read/considered.

* Computer Science Base

** Deep Learning

   + Long Short-Term Memory
     - Status :: to-read
* Can't Decide The Category

  + Parsing Graphs With Hyperedge Replacement Grammars
    - Abstract :: Hyperedge replacement grammar (HRG) is a formalism for generating and transforming graphs that has potential applications in natural language understanding and generation. A recognition algorithm due to Lautemann is known to be polynomial-time for graphs that are connected and of bounded degree. We present a more precise characterization of the algorithm’s complexity, an optimization analogous to binarization of context-free grammars, and some important implementation details, resulting in an algorithm that is practical for natural-language applications. The algorithm is part of Bolinas, a new software toolkit for HRG processing.
    - Status :: skimmed

  + Enriching Word Vectors With Subword Information
    - Abstract :: Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation,
especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.
    - Status :: not-read

  + Natural Language Processing (Almost) From Scratch
    - Abstract :: We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge.  Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data.   This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.
    - Status :: not-read
 
* Rule Mining

  + TagMiner: A Semisupervised Associative POS Tagger Effective for Resource Poor Languages
    - Abstract :: We present here, TagMiner, a data mining approach for part-of-speech (POS) tagging, an important Natural language processing (NLP) classification task. It is a semi-supervised associative classification method for POS tagging. Existing methods for building POS taggers require extensive domain and linguistic knowledge and resources. Our method uses combination of a small POS tagged corpus and a raw untagged text data as training data to build the classifier model using association rules. Our tagger works well with very little training data also. The use of semi-supervised learning provides the advantage of not requiring a large high quality tagged corpus. These properties make it especially suitable for resource poor languages. Our experiments on various resource-rich, resource-moderate and resource-poor languages show good
performance without using any language specific linguistic information. We note that inclusion of such features in our method may further improve the performance. Results also show that for smaller training data sizes our tagger performs better than state-of-the-art CRF tagger using same features as our tagger.
    - Status :: read
* Keyphrase Extraction

   + Automatic Keyphrase Extraction: A Survey of the State of the Art
     - Abstract :: While automatic keyphrase extraction has been  examined  extensively,  state-of-the-art performance on this task is still much lower than that on many core natural language processing tasks.  We present a survey  of  the  state  of  the  art  in  automatic keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead.
    - Status :: read

* Question-Answering

** Question-Answering Sytems

  + Hand in Glove: Deep Feature Fusion Network Architectures for Answer Quality Prediction in Community Question Answering
     - Abstract :: Community Question Answering (cQA) forums have become a popular medium for soliciting answers to specific user questions from experts and experienced users in a given topic. However, for  a  given  question,  users  sometimes  have  to  sift  through  a  large  number  of  low-quality  or irrelevant answers to find out the answer which satisfies their information need. To alleviate this, the problem of Answer Quality Prediction (AQP) aims to predict the quality of an answer posted in response to a forum question.  Current AQP systems either learn models using - a) various hand-crafted features (HCF) or b) Deep Learning (DL) techniques which automatically learn the feature representations. In this paper, we propose a novel approach for AQP known as - “Deep Feature Fusion Network (DFFN)” which combines the advantages of both hand-crafted features and deep learning based systems.  Given a question-answer pair along with its metadata, a DFFN architecture independently - a) learns features using the Deep Neural Network (DNN) and b) computes hand-crafted features leveraging various external resources and then combines them using a fully connected neural network trained to predict the quality of the given answer.  DFFN is an end-end differentiable model and trained as a single system. We propose two different DFFN architectures which vary mainly in the way they model the input question/answer pair - a) DFFN-CNN which uses a Convolutional Neural Network (CNN) and b) DFFN-BLNA which uses a Bi-directional LSTM with Neural Attention (BLNA). Both these proposed variants of DFFN (DFFN-CNN and DFFN-BLNA) achieve state-of-the-art performance on the standard SemEval-2015 and SemEval-2016 benchmark datasets and outperforms baseline approaches which individually employ either HCF or DL based techniques alone.
    - Status :: skimmed

  + Together We Stand: Siamese Networks for Similar Question Retrieval
     - Abstract :: Community  Question  Answering  (cQA) services  like  Yahoo! Answers,  Baidu Zhidao,   Quora,   StackOverflow etc. provide  a  platform  for  interaction  with experts  and  help  users  to  obtain  precise and  accurate  answers  to  their  questions. The  time  lag  between  the  user  posting  a question  and  receiving  its  answer  could be  reduced  by  retrieving  similar  historic questions  from  the  cQA  archives. The main challenge in this task is the “lexico- syntactic” gap between the current and the previous questions.  In this paper, we propose  a  novel  approach  called “Siamese Convolutional Neural Network for cQA (SCQA)” to  find  the  semantic  similarity between the current and the archived questions. SCQA  consist  of  twin  convolutional neural networks with shared parameters and a contrastive loss function joining them. SCQA  learns  the  similarity  metric  for question-question pairs by leveraging the question-answer pairs available in cQA forum archives. The model projects semantically similar question pairs nearer to each other  and  dissimilar  question  pairs  farther away from each other in the semantic space. Experiments on large scale real-life “Yahoo! Answers” dataset reveals that SCQA  outperforms  current  state-of-the-art approaches based on translation models, topic models and deep neural network based  models  which  use  non-shared  parameters.
    - Status :: skimmed

  + Simple and Effective Multi-Paragraph Reading Comprehension
    - Abstract :: We consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs. We sample multiple paragraphs from the documents during training, and use a shared-normalization training objective that encourages the model to produce globally correct output. We combine this method with a state-of-the-art pipeline for training models on document QA data. Experiments demonstrate strong performance on several document QA datasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system. 
    - Status :: not-read

** Query Parsing

   + Syntactic Parsing of Web Queries With Question Intent
     - Abstract :: Accurate automatic processing of Web queries is important  for  high-quality  information  retrieval  from the  Web.   While  the  syntactic  structure  of  a  large portion  of  these  queries  is  trivial,  the  structure  of queries with question intent is much richer.  In this paper  we  therefore  address  the  task  of  statistical syntactic  parsing  of  such  queries.    We  first  show that the standard dependency grammar does not account for the full range of syntactic structures manifested  by  queries  with  question  intent.   To  alleviate this issue we extend the dependency grammar to account for segments – independent syntactic units within  a  potentially  larger  syntactic  structure.   We then propose two distant supervision approaches for the task.  Both algorithms do not require manually parsed queries for training. Instead, they are trained on millions of (query, page title) pairs from the Community Question Answering (CQA) domain,  where the CQA page was clicked by the user who initiated the query in a search engine. Experiments on a new treebank consisting of 5,000 Web queries from the CQA domain, manually parsed using the proposed grammar, show that our algorithms outperform alternative approaches trained on various sources: tens of thousands of manually parsed OntoNotes sentences, millions of unlabeled CQA queries and thousands of manually segmented CQA queries
    - Status :: read

* Semantic Representations

** General

   + State of the Art in Semantic Representation
     - Status :: read

** AMR

   + Abstract Meaning Representation For Sembanking
     - Abstract :: We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sentences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it.
     - Status :: read

   + A Transition-Based Algorithm for AMR Parsing
     - Abstract :: We  present  a  two-stage  framework  to  parse a sentence  into  its  Abstract  Meaning  Representation (AMR). We first use a dependency parser  to  generate  a  dependency  tree  for  the sentence. In  the  second  stage,  we  design a novel transition-based algorithm that transforms the dependency tree to an AMR graph. There  are  several  advantages  with  this  approach.   First,  the dependency parser can be trained on a training set much larger than the training set for the tree-to-graph algorithm, resulting in a more accurate AMR parser overall.  Our parser yields an improvement of 5% absolute in F-measure over the best previous result.  Second, the actions that we design are linguistically intuitive and capture the regularities in the mapping between the dependency structure and the AMR of a sentence.  Third, our parser runs in nearly linear time in practice in spite of a worst-case complexity of O(n2)
      - Status :: skimmed
** ELMo

   + Deep Contextualised Word Representations
     - Abstract :: We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned func-
tions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.
    - Status :: read

** SRL

   + Deep Semantic Role Labelling: What Works and What's Next
     - Status :: to-read

** UDS

   + Universal Decomposition Semantics on Universal Dependencies
     - Abstract :: We present a framework for augmenting data sets from the Universal Dependencies project with Universal Decompositional Semantics. Where  the  Universal  Dependencies  project aims  to  provide  a  syntactic  annotation  standard that can be used consistently across many languages  as  well  as  a  collection  of  corpora that use that standard, our extension has similar aims for semantic annotation. We describe results  from  annotating  the  English  Universal Dependencies treebank, dealing with word senses, semantic roles, and event properties.
    - Status :: read

* Semantic Parsing


  + Universal Semantic Parsing
    - Abstract :: Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications.    Recent  work  shows  that  semantic  parsing  can  be  accomplished  by transforming syntactic dependencies to logical  forms.   However,  this  work  is  limited  to  English,  and  cannot  process  dependency  graphs,  which  allow  handling complex phenomena such as control.  In this work,  we introduce UDEPLAMBDA, a semantic interface for UD, which maps natural  language  to  logical  forms  in  an almost language-independent fashion and can process dependency graphs.  We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation. Results show that UDEPLAMBDA outperforms strong baselines across languages and datasets.  For English, it achieves a 4.9 F1 point improvement  over  the  state-of-the-art  on  Graph-Questions.
    - Status :: not-read

  + Transforming Dependency Structures to Logical Forms for Semantic Parsing
    - Abstract :: The  strongly  typed  syntax  of  grammar  formalisms such as CCG, TAG, LFG and HPSG offers a synchronous framework for deriving syntactic structures and semantic logical forms. In contrast—partly due to the lack of a strong type system—dependency structures are easy to annotate and have become a widely used form of syntactic analysis for many languages. However, the lack of a type system makes a formal mechanism for deriving logical forms from dependency structures challenging.  We address  this  by  introducing  a  robust  system based on the lambda calculus for deriving neo-Davidsonian logical forms from dependency trees.  These logical forms are then used for semantic parsing of natural language to Free- base.  Experiments on the Free917 and Web-Questions datasets show that our representation is superior to the original dependency trees and that it outperforms a CCG-based representation on this task. Compared to prior work, we obtain the strongest result to date on Free917 and competitive results on WebQuestions
    - Status :: not-read

* Keyphrase Extraction

  + 
