#+TITLE: Class Notes - 22/1/18
#+AUTHOR: meow meow motherfuckers

# i am rather bored in this class, yes

* Doodles

      ~~~           ~~~
~~~          ~~~          ~~~
   bbbb
   bvvvb      /vvv\
   bvvvvb    /xxxxx\   ~~~
   bvvvvvb  /xxxxxxx\         ~~~   
   bvvvvvvbvxxxxxxxxx>>
   bvvvvvvbvxxxxxxxxx>>   ~~~
   bvvvvvb  \xxxxxxx/
   bvvvvb    \xxxxx/          ~~~
   bvvvb      \vvv/   ~~~
   bbbb
~~~         ~~~          ~~~
     ~~~          ~~~

* Until Now

We looked at:

/Where/ is information.
/How/ is information coded in language.

Information is there in:
    - Meaning
    - Ontology of the word

Eg:

{man, woman} belongs to :Human Being:

If we want to separate them, we define a feature (Eg: +feminity) that differentiates them.
* Meaning In The Relation Between Words

Relation between words also holds meanings.

** Prepositional Phrase Attachment Problem

Eg: "I saw a boy in a blue shirt."

Here, "blue shirt" is clearly connected to 'boy'.

However, "I saw the boy from the window."

"from the window" --> saw

One area where a parser fails is PP attachment is that a PP can be attached to either a verb or a noun, and the structure of both sentences is the same.

Therefore, the difference is not in the structure but rather in the relation between the words: (boy <-- shirt) vs. (saw <-- room).

Since all parsers till date use only syntactic and not semantic information, in cases like this the parser will fail.

** Conjunction Attachment Problem

Parser also fails when there is more than one conjunction in the sentence, also w.r.t which clause connects to which clause (conjunction attachment)

** WSD Changes as Society Changes

Eg: 'Jill and Mary are mothers.'

Initially: they are mothers of different kids.
With the existence of mainstream queer culture: they could also be mothers of the same kid.

# won't data-driven systems take care of this?

** Ambiguity At The Sentence Level

Can only be resolved using wider context of the discourse.

* How To Represent Word-Relational Information

So that it can be computable.

# PREDICATE LOOOOGIIIIIC
# *LOUD SINGING*

# KW: do shut up, ciel

** VerbNet

# how many goddamn nets are there
# A: ~~~net~~~

Framenet represents different /situations/.

Verbnet has all entries of /verbs/.

All of these resources are internally linked.

# L: why not just record the entire damned language in an ontology

# KW: what, Lima, do you think annotators are trying to do?
* Semantic Representation of a Larger Sentence

** Properties

*** Unambiguity 

Sentences are generally ambiguous. The representation should be unambiguous.
*** Explicitness  

Semantic representation should be 'explicit'
     - Eg: "*I* want to go to my friend's house"
            Representation: (I -[Nsubj]-> want)<-[Pred]-(I -[Nsubj]-> go)
           "I want *him* to go to my friend's house."
            Representation: (I -[Nsubj]->want)<-[Pred]-(go <-[Nsubj]-him)
Also additionally:

  + "I want /him to go/" : two clauses
  + "I want /him from the core of my heart/" : One clause

In natural language sentences, information is dropped/ *elided* (known as *ellipsis) 

This is needed because, for example, to answer questions based on implicit info, eg in the above sentence we can ask the question "Who will go to the friend's house."

We need explicit representations because machine be stupid. Machine cannot get implicit representation. Machine needs everything made explicit to it. Machine shares a lot in common with Aspies like me.
   
*** Canonicalness

"Conicalness." - Nemani, 2k18

Eg: one semantic representation can be put in multiple forms. (Simply put: different sentences mean the same thing.)

This is useful for tasks like *summarisation* and *information extraction*.

*Sentence similarity measure* is a thing.

"These are very very happening areas of research today, in NLP." - Soma, 2k18

** The Fault In Our Parsers

The problem with too much detail for the parser is that:

    - Parser becomes slow
    - Errors increase

This is why, as said above, we have different levels of sophistication in our parsers.


Eg: Stanford Core NLP Parser

Has different versions, and each version gives different levels of info, because we don't need, say, the above level of detail for all tasks.

** Predicate Logic

Why hello there my beauty.

However, predicate logic is not powerful enough to express all relations. Eg: "Most children are happy.".

There have been developed powerful systems beyond predicate logic.

# ask soma 
# it would be cute AF to extend our smol parser system to something beyond predicate logic

** Semantic Relations

   + Entailment ::
     - If P is true means that Q is true
     - Predicate logic representation: /implication/
** Possible World

When we are modelling sentences, we /assume/ a world where, for example, certain things are possible and certain things aren't.

"I don't care if they call me crazy
We can live in a world that we designed!" - PT Barnum, 2k18. TIL that The Greatest Showman was an early semanticist.

When we deal with a natural language discourse, we deal with several /worlds/ - the world of the speaker, the physical world, the world of the hearer...

Therefore, NL Semantics is a very complex task.

** Inference Systems

Current inference systems are very crude, and can handle only a few subtypes of questions (Fact questions- why, how, etc..)

* Sub-Event Analysis

# my notes are rather sparse here
# sorry was doodling

Eg: Ram ne sita ko hasaya.

Hasaya --> /sita ka hasna/ is a sub-event.

This is but one example of how words, and language, have layered information.

** Representation of Action

Functions, etc are not sufficient to represent action. Therefore any semantic representation has to have action representation.

** Sub-Event Analysis

Any semantic representation must also account for sub-event analysis as various levels.
** Event Quantification

Example:

"John killed Mary." 

Representation as theta roles:

    E e (killing(e), agent(e, John), Theme(e, Mary))

* Extended Reading and Homework

  - Paper by /Manning/ on PPAP
  - Jurafsky - Event Semantics/Semantic Representation vaala chapter- 19 && 20 - read and ask her questions
  - Data-driven NLP and knowledge-rich NLP???
  - Mayybe share notes on predicate logic???
  - read up on theta roles
